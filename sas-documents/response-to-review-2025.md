# Response to End of Year Review 2025

**Employee:** Joana Socrates
**Role:** Engineering Manager, Foundations
**Manager:** Nick Stevenson
**Date:** January 2026

---

## Purpose of This Response

I am submitting this response to provide context and clarification on several points in my end-of-year review. I believe some assessments do not reflect the full picture and I wish to ensure my perspective is documented for the record.

---

## 1. Response to Example 1: Pricing Service Escalation

**Review statement:** *"Recently (October) you again raised a request for more headcount, but you did it as an escalation to HR co-ing members of your team before you had taken real action to solve the problem yourself and without discussing with me first."*

**My response:**

This characterisation is inaccurate. I raised the Pricing Service resource concerns with Nick multiple times in meetings and emails prior to the October escalation. The timeline was as follows:

- I raised the need for tech-protected resources in multiple meetings with Nick
- Nick's response was to tell me to "go ahead and keep the resources"
- This did not address the core issue: I needed **cross-functional alignment with Product**, which requires director-level involvement
- I cannot unilaterally negotiate with Product leadership on resource allocation - that is a director responsibility
- After repeated attempts to get action, I escalated because the situation was affecting delivery and engineer wellbeing

Additionally, when a meeting with Product (Rose) was finally arranged, Nick did not communicate the agreed percentage for protected technical work. I had to raise it myself in the meeting, at which point Rose informed me that Josh had not agreed to it because it had never been communicated to him.

**For the record:** The escalation occurred after multiple attempts to resolve the issue through my direct manager, not as a bypass of proper channels.

### Supporting Evidence: Email Thread dated 22-23 October 2025

I have retained the email thread "Urgent - Foundations (Pricing) Headcount Stability" which documents:

**My email to Nick and Sue (22 October 2025 at 12:50):**
> "As I've raised couple of times since joining Foundations, the pricing team has less resources than the feature requests they receive."

> "I mentioned this plan to Kyri (who was happy with it) and to Nick (who did neither agree nor disagreed)."

> "As this is beyond my decision influence (cross team / SAS wide) I ask one of you to decide and inform before we break and impact the whole company."

**My follow-up email (22 October 2025 at 12:19):**
> "To sum up I've discussed this topic this with James, Nick, Kyri and Iain. And mentioned it to Sue and Scott"

**Nick's response (22 October 2025 at 19:38):**
> "I would still like to see FDT solve this problem internally but I'm open to discussing it further with James and Joana in 1 to 1s this week."

**My response (23 October 2025):**
> "It is not possible to solve this internal in Foundation, without Kyri and Vlad FDT will be 2 headcount below originally planned. And with crucial work in Q4 and 2026"

This email thread demonstrates:
1. I explicitly stated I had already discussed this with Nick prior to the email
2. Nick's response was to defer to future 1:1s rather than make a decision
3. I clearly explained why this could not be solved internally (cross-team decision)
4. The escalation included Nick directly - it was not a bypass

---

## 2. Response to Example 2: Communication and Self-Assessment Links

**Review statement:** *"Your self-assessment for this review is a good example - there are many links that are broken... Such things ought to be mostly right-first-time if you are taking enough care."*

**My response:**

I submitted my self-assessment more than one month before the review deadline. During this period, Nick and I had weekly one-on-one meetings. At no point were the broken links mentioned, nor was I asked to clarify or fix any issues.

I would have been happy to address any access or formatting issues had they been raised in our regular meetings. Raising this on the final day before publication, when there was no opportunity to correct it, and then using it as evidence of a "performance issue" does not reflect a constructive feedback approach.

**For the record:** I had multiple opportunities to fix these issues if they had been flagged in a timely manner. The characterisation of broken links as evidence of poor communication is not consistent with the available time to provide feedback.

---

## 3. Response to Goal Evaluation: New Joiner Onboarding

**Review statement:** *"I have some concern about the achieved value of this goal and the minimal effort needed to achieve it."*

**My response:**

This goal was set during my transition from Accounting to Foundations, when the company was actively hiring across multiple roles (managers, TPMs, PMs, engineers). Creating new joiner resources was a relevant priority at that time.

Hiring significantly decreased mid-year. Given this context change, I deprioritised new joiner content to focus on more pressing delivery needs. This was a pragmatic decision based on business reality.

I received no feedback from Nick throughout the year suggesting I should redirect this goal or that my approach was problematic. When I manage my own direct reports' goals, I actively check in on progress and help them adapt goals when organisational context changes. This did not happen for my goals.

**For the record:** The criticism of "minimal effort" does not account for the organisational context change (reduced hiring) that made this goal less relevant, nor the absence of feedback during the year.

---

## 4. Response to Documentation Concerns

**Review statement:** The Glossary is characterised as "probably a couple of hours work at most" with "only 6 unique viewers."

**My response:**

The low viewer count for the Glossary is directly related to the reduced hiring mentioned above. Fewer new joiners means fewer people using new joiner resources.

More broadly, I am concerned that documentation is being framed as a performance issue when my documentation output is among the highest of the Engineering Managers. I have created and shared:

- Datadog configuration instructions for repositories
- Repository management guidance and standards
- Capacity planning templates (reusable across teams)
- Tool documentation with video tutorials
- Engineering Manager guides for onboarding new people managers

I have not received specific, actionable guidance on how to improve my documentation, nor has this standard been applied consistently across all Engineering Managers.

**For the record:** I would welcome specific examples and guidance on what "good" documentation looks like and how my output compares to expectations and peers.

---

## 5. Response to Incident Management Assessment

**Review statement:** Statistics are provided (54% response within SLA, 71% resolved within SLA) in the context of concerns about incident management.

**My response:**

I have reviewed the Power BI incident management data for comparable periods and can confirm that metrics **significantly improved** under my management.

My approach to incident management was strategic:
- In my previous team (Finance), I managed most incidents personally, which created a single point of failure
- When I was unavailable (holidays, illness, performance reviews), incident response quality suffered
- In Foundations, I deliberately trained engineers to handle incident management independently
- This included documenting protocols, training on triage and user communication, and distributing responsibility across the rota
- The goal was to build sustainable capability rather than personal dependency

### Supporting Evidence: Power BI Incident Metrics Comparison

**Before my management (Sep-Dec 2024):**
- Response SLA Met: **48.84%**
- Resolution SLA Met: **79.07%**
- Resolution SLA Breaches: 6

**Under my management (Sep-Dec 2025):**
- Response SLA Met: **78.72%**
- Resolution SLA Met: **95.74%**
- Resolution SLA Breaches: 2

**Improvement:**
| Metric | Change |
|--------|--------|
| Response SLA | +29.88 percentage points |
| Resolution SLA | +16.67 percentage points |
| SLA Breaches | -67% reduction |

This improvement occurred despite an increase in incident volume (43 â†’ 47 incidents), which is expected with product growth and more users.

**For the record:** The objective data from company systems demonstrates that my strategic approach to incident management resulted in significant, measurable improvement across all key metrics. I request that this improvement be acknowledged in my performance assessment.

---

## 6. Response to "Empowering and Inspiring" Behaviours Feedback

**Review statement:** *"I would like you to pay more attention to high standards and building trust with me and SAS leadership."*

**My response:**

I am committed to building trust and maintaining high standards. However, I wish to note that trust is built through consistent, fair, and supportive interactions over time.

I have concerns about several incidents this year that have affected my confidence and willingness to share work openly:

- Public criticism of my documentation in a meeting with other Engineering Managers present (first half of 2025)
- Feedback from my 18 direct reports stating I communicate "directly, clearly, and objectively" - which was acknowledged but characterised as "different from his view"
- Suggestions to use AI tools (Copilot) to improve writing, without support in obtaining access to those tools

I remain committed to improving and would welcome specific, actionable feedback delivered in a constructive manner.

---

## 7. Feedback from Direct Reports

I proactively gathered feedback from my 18 direct reports at mid-year. A summary of this feedback indicated that my direct reports view my communication as direct, clear, and objective.

I note that this positive feedback is not reflected in my review, and was verbally characterised as "different from his view." I would like to understand how feedback from 18 professionals who work with me daily is weighted against my manager's assessment.

---

## 8. Request for Clarification

I respectfully request clarification on the following:

1. **Incident metrics:** Do the statistics in my review represent an improvement or decline compared to previous periods?

2. **Documentation standards:** What specific improvements are expected, and how does my output compare to other Engineering Managers?

3. **Communication improvement:** What specific, actionable steps are recommended beyond general guidance to "consider the audience"?

4. **Goal-setting process:** How should goals be adapted when organisational context changes significantly mid-year?

5. **Feedback timing:** How can I receive more timely feedback throughout the year so issues can be addressed before the annual review?

---

## Summary

I accept responsibility for areas where I can improve and am committed to doing so. However, I believe this review contains assessments that do not reflect the full context, and I wish to ensure my perspective is documented.

I am concerned about:
- Criticism raised at year-end that could have been addressed in weekly meetings
- Goal evaluation that does not account for organisational context changes
- Positive evidence (direct report feedback, improved metrics) that appears to be dismissed
- The framing of escalations (which occurred after repeated attempts to resolve issues) as failures of self-accountability

I remain committed to my role, my team, and the company's success. I welcome constructive dialogue on how to address these concerns and improve our working relationship going forward.

---

**Submitted by:** Joana Socrates
**Date:** [Date]

---

*This response is submitted for the record as part of the 2025 performance review process.*
